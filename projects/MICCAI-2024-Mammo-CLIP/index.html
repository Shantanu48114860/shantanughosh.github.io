<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="Mammo-CLIP: Enhancing Mammography with Vision Language Models">
    <meta name="keywords" content="Mammography, Breast Cancer, AI, Vision Language Models, CLIP">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Mammo-CLIP</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/figures/bu-logo.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <style>
        .title-text {
            display: inline-block;
            word-break: break-word;
            hyphens: auto;
        }

        @media screen and (max-width: 768px) {
            .image-text-container {
                flex-direction: column;
                align-items: center;
            }

            .image-text-container img {
                margin-left: 0;
                margin-right: 0;
                margin-bottom: 20px;
                max-width: 100%;
                height: auto;
            }

            .title-text {
                font-size: 2rem;
            }

        }
    </style>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">
                        <div class="title-text">Mammo-CLIP: A Vision Language Foundation
                            Model to Enhance Data Efficiency and Robustness in Mammography
                        </div>
                    </h1>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                          <a href="https://shantanu-ai.github.io/">Shantanu Ghosh</a><sup>1</sup>,</span>
                        <span class="author-block">
                          <a href="https://www.bumc.bu.edu/camed/profile/clare-poynton/">Clare B. Poynton</a><sup>2</sup>,</span>
                        <br>
                        <span class="author-block">
                          <a href="https://www.thevislab.com/lab/doku.php">Shyam Visweswaran</a><sup>3</sup>,</span>
                        <span class="author-block">
                          <a href="https://www.batman-lab.com/">Kayhan Batmanghelich</a><sup>1</sup>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup><span
                                style="color: #0A3D62;">ECE, Boston University,</span></span>
                        <span class="author-block"><sup>2</sup><span
                                style="color: #0A3D62;"> Boston University Chobanian & Avedisian School of Medicine,
                        </span></span>
                        <span class="author-block"><sup>3</sup><span
                                style="color: #0A3D62;"> ISP, University of Pittsburgh</span></span>
                    </div>


                    <div class="is-size-4 publication-authors">
                        <span class="author-block"><b>27th INTERNATIONAL CONFERENCE ON MEDICAL IMAGE COMPUTING
                            AND COMPUTER ASSISTED INTERVENTION (MICCAI) 2024 </b><br/>
                            (Early Accept top 11%)
                        </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                                <a href="https://arxiv.org/pdf/2405.12255.pdf"
                                   class="external-link button is-normal is-rounded is-dark">
                                  <span class="icon">
                                      <i class="fas fa-file-pdf"></i>
                                  </span>
                                  <span>Paper</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="https://arxiv.org/abs/2405.12255"
                                   class="external-link button is-normal is-rounded is-dark">
                                          <span class="icon">
                                              <i class="ai ai-arxiv"></i>
                                          </span>
                                          <span>arXiv</span>
                                </a>
                            </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                                <a href="https://github.com/batmanlab/Mammo-CLIP"
                                   class="external-link button is-normal is-rounded is-dark">
                                  <span class="icon">
                                      <i class="fab fa-github"></i>
                                  </span>
                                  <span>Code</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -55px; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div style="text-align: justify;">
                <h2 class="title is-3">Revolutionizing Breast Cancer Detection with AI</h2>
                <p>Breast cancer is a leading cause of death among women worldwide, with early detection being crucial
                    for
                    effective treatment. However, the development of robust computer-aided diagnosis (CAD) systems is
                    often
                    limited by the availability of large, diverse annotated datasets. Mammo-CLIP addresses these
                    challenges
                    by introducing a Vision Language Model specifically trained on paired mammogram images and reports,
                    significantly enhancing the robustness and data efficiency of CAD systems.</p><br>
                <p>Mammo-CLIP employs multi-view supervision, data augmentation, and a novel feature attribution method
                    called Mammo-FActOR, to provide interpretable and highly accurate results. By leveraging
                    high-resolution
                    images and diverse training data, Mammo-CLIP excels in detecting and localizing critical
                    mammographic
                    attributes such as masses and calcifications.</p><br>
                <p>Explore our results and see how Mammo-CLIP sets a new standard in mammography AI.</p><br>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -10%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <h2 class="title is-3">Mammo-CLIP Schematic Overview</h2>
            <div style="text-align: justify;">
                <p>The schematic below illustrates the core components of Mammo-CLIP, including image-text
                    augmentation, dataset augmentation, pretraining strategy, and feature attribution using
                    Mammo-FActOR. These components are crucial for enhancing the model's ability to classify and
                    localize mammographic features with high accuracy.</p><br/>
            </div>
            <figure style="width: 95%; margin: 0 auto;">
                <img src="./static/figures/Mammo-CLIP-schematic.png" alt="Mammo-CLIP Schematic"
                     style="width: 100%; height: auto;"/>
                <figcaption style="text-align: center; font-size: 14px; color: #555;">
                    Fig. 1. Schematic view of our method. (a) Image-text augmentation for Multi-View Supervision (MVS).
                    (b) Dataset augmentation by synthesizing reports using image-attribute datasets.
                    (c) Mammo-CLIP pretraining strategy.
                    (d) Feature attribution using Mammo-FActOR.
                </figcaption>
            </figure>
        </div>
    </div>
</section>


<!--<section class="hero teaser" style="margin-top: -5px;">-->
<!--    <div class="container is-max-desktop">-->
<!--        <div class="hero-body">-->
<!--            <video autoplay muted controls loop style="width: 100%;">-->
<!--                <source src="static/figures/mammo_clip_analysis.mp4" type="video/mp4">-->
<!--                Your browser does not support the video tag.-->
<!--            </video>-->
<!--        </div>-->
<!--    </div>-->
<!--</section>-->

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Mammo-CLIP Optimization</h2>
                    <div class="content" style="text-align: justify;">
                        <p>Mammo-CLIP is a Vision Language Model specifically designed for mammography. The core
                            objective
                            of Mammo-CLIP is to align visual representations from mammogram images with corresponding
                            textual descriptions found in radiology reports. This alignment is achieved by employing
                            separate encoders for images and text. The model is trained to ensure that similar images
                            and
                            texts have closely aligned representations in a shared feature space. This approach improves
                            the
                            model's ability to accurately interpret and classify mammographic findings, leading to more
                            reliable and interpretable results.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Instance and Dataset Augmentation</h2>
                    <div class="content" style="text-align: justify;">
                        <p>To enhance the robustness and generalizability of Mammo-CLIP, both instance-level and
                            dataset-level augmentation techniques are utilized. Instance augmentation involves creating
                            multiple modified versions of each mammogram image and its corresponding report. These
                            modifications include changes such as flipping, rotation, and cropping of images, as well as
                            slight rephrasing of the text. Dataset augmentation, on the other hand, synthesizes entirely
                            new
                            data pairs by combining images and text from different sources within the dataset. These
                            strategies help the model learn more diverse patterns, making it better suited to handle
                            variations in real-world data.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Mammo-FActOR: Interpretable AI for Mammography</h2>
                    <div class="content" style="text-align: justify;">
                        <p>Mammo-FActOR is an interpretability module integrated within Mammo-CLIP. This module
                            maps the
                            visual features extracted from mammograms to specific textual attributes found in
                            radiology
                            reports. By doing so, Mammo-FActOR provides a clearer understanding of how the model
                            interprets
                            different regions of the image in relation to the findings described in the text.
                            This
                            enhanced
                            interpretability allows radiologists to better understand the model’s
                            decision-making
                            process,
                            increasing the trustworthiness and transparency of the system.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Results and Impact</h2>
                    <div class="content" style="text-align: justify">
                        <p>Mammo-CLIP demonstrates strong performance across several public datasets, significantly
                            outperforming existing models in both classification and localization tasks. The model's
                            robustness
                            and data efficiency make it a valuable asset in the early detection of breast cancer,
                            potentially
                            saving lives by enabling faster and more accurate diagnoses.</p>
                    </div>
                    <!--                    <video autoplay muted controls loop style="width: 100%;">-->
                    <!--                        <source src="static/figures/mammo_clip_results.mp4" type="video/mp4">-->
                    <!--                        Your browser does not support the video tag.-->
                    <!--                    </video>-->
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Classification Performance on RSNA Dataset</h2>
                    <div style="text-align: justify;">
                        <p>The plot below shows the classification performance of various models on the RSNA dataset to
                            classify
                            malignancy. The models were evaluated using AUC scores across different training settings
                            including
                            zero-shot, fine-tuning with 10%, 50%, and 100% of the data, and linear probe with 100% of
                            the data.</p>
                        <br/>
                    </div>
                    <figure style="width: 95%; margin: 0 auto;">
                        <img src="./static/figures/cancer-auc.png" alt="Classification Performance on RSNA Dataset"
                             style="width: 100%; height: auto;"/>
                        <!--                        <figcaption style="text-align: center; font-size: 14px; color: #555;">-->
                        <!--                            Fig. 2. Classification performance comparison on RSNA dataset using AUC scores.-->
                        <!--                        </figcaption>-->
                    </figure>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">AUC Performance for Calcification, Mass, and Density on VinDr dataset</h2>
                    <div style="text-align: justify;">
                        <p>This plot compares the AUC performance of various models on calcification, mass, and density
                            classification tasks. The performance is reported across zero-shot, linear probe (10%, 50%,
                            100%
                            data), and fine-tune (100% data) settings, providing insights into the model's robustness
                            across
                            different conditions.</p>
                        <br/>
                    </div>
                    <figure style="width: 95%; margin: 0 auto;">
                        <img src="./static/figures/concept-auc.png"
                             alt="AUC Performance for Calcification, Mass, and Density"
                             style="width: 100%; height: auto;"/>
                        <!--                        <figcaption style="text-align: center; font-size: 14px; color: #555;">-->
                        <!--                            Fig. 3. AUC performance comparison for calcification, mass, and density classification-->
                        <!--                            tasks.-->
                        <!--                        </figcaption>-->
                    </figure>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Localization Performance on VinDr Dataset</h2>
                    <div style="text-align: justify;">
                        <p>The following plot presents the localization performance (mAP) on the VinDr dataset. The
                            evaluation
                            compares different models under various training conditions including freeze encoder,
                            fine-tuning
                            with 10%, 50%, and 100% of the data.</p>
                        <br/>
                    </div>
                    <figure style="width: 95%; margin: 0 auto;">
                        <img src="./static/figures/detection-map.png" alt="Localization Performance on VinDr Dataset"
                             style="width: 100%; height: auto;"/>
                        <!--                        <figcaption style="text-align: center; font-size: 14px; color: #555;">-->
                        <!--                            Fig. 4. Localization performance comparison (mAP) on VinDr dataset.-->
                        <!--                        </figcaption>-->
                    </figure>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Mammo-FActOR Interpretability</h2>
                    <div style="text-align: justify;">
                        <p>This figure showcases the interpretability of Mammo-FActOR. The ground-truth regions for mass
                            and
                            calcification are compared against the model’s predictions, visualized through heatmaps that
                            highlight
                            the model’s focus areas.</p>
                        <br/>
                    </div>
                    <figure style="width: 95%; margin: 0 auto;">
                        <img src="./static/figures/Mammo-Factor.png" alt="Mammo-FACtoR Interpretability"
                             style="width: 100%; height: auto;"/>
                        <figcaption style="text-align: center; font-size: 14px; color: #555;">
                            Ground-truth and Mammo-FActOR prediction visualizations for mass and calcification.
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Weakly-Supervised Localization Results</h2>
                    <div style="text-align: justify;">
                        <p>The bar plot below compares the Intersection over Union (IoU) performance of two Mammo-CLIP
                            variants
                            for mass and calcification detection on the VinDr dataset. The IoU is reported at thresholds
                            of 0.25
                            and 0.50, showcasing the models' effectiveness in weakly-supervised localization tasks.</p>
                        <br/>
                    </div>
                    <figure style="width: 95%; margin: 0 auto;">
                        <img src="./static/figures/weak-supervised.png" alt="Weakly-Supervised Localization Results"
                             style="width: 100%; height: auto;"/>
                        <figcaption style="text-align: center; font-size: 14px; color: #555;">
                            IoU comparison for weakly-supervised localization of mass and calcification.
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title is-3">Future Directions</h2>
                    <div class="content" style="text-align: justify;">
                        <p>We are excited to continue developing Mammo-CLIP by exploring the integration of vision
                            transformers
                            and cross-attention mechanisms, which have the potential to further enhance the model's
                            performance.
                            We also aim to expand the dataset to include more diverse mammogram images and reports,
                            ensuring
                            that Mammo-CLIP remains at the forefront of AI in healthcare.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="margin-top: -7%; margin-bottom:-10px">
    <div class="container is-max-desktop content">
        <div class="hero-body">
            <div class="columns is-centered">
                <div class="content">
                    <h2 class="title-is-3">BibTeX</h2>
                    <pre><code>
                            @article{ghosh2024mammo,
                              title={Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography},
                              author={Ghosh, Shantanu and Poynton, Clare B and Visweswaran, Shyam and Batmanghelich, Kayhan},
                              journal={arXiv preprint arXiv:2405.12255},
                              year={2024}
                            }
                    </code></pre>
                </div>
            </div>
        </div>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <a class="icon-link"
               href="https://arxiv.org/abs/2305.17303">
                <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs"
               class="external-link" disabled>
                <i class="fab fa-github"></i>
            </a>
        </div>
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p style="text-align: center;">
                        Copyright © Batman Lab, 2023. Feel free to use this <a
                            href="https://github.com/shantanu-ai/shantanu-ai.github.io/tree/main/projects/MICCAI-2023-MoIE-CXR">website's
                        template</a>, adapted from
                        <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>